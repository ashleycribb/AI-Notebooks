{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-anthropic langchain-core anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4-357GaUJnW",
        "outputId": "801a8757-3d32-49aa-a04a-c8161ebb8bf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain.tools import Tool\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "import sys\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "from typing import Dict, Any, List"
      ],
      "metadata": {
        "id": "n7ezWTciXhih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PythonREPLTool:\n",
        "    def __init__(self):\n",
        "        self.globals_dict = {\n",
        "            '__builtins__': __builtins__,\n",
        "            'json': json,\n",
        "            're': re\n",
        "        }\n",
        "        self.locals_dict = {}\n",
        "        self.execution_history = []\n",
        "\n",
        "    def run(self, code: str) -> str:\n",
        "        try:\n",
        "            old_stdout = sys.stdout\n",
        "            old_stderr = sys.stderr\n",
        "            sys.stdout = captured_output = io.StringIO()\n",
        "            sys.stderr = captured_error = io.StringIO()\n",
        "\n",
        "            execution_result = None\n",
        "\n",
        "            try:\n",
        "                result = eval(code, self.globals_dict, self.locals_dict)\n",
        "                execution_result = result\n",
        "                if result is not None:\n",
        "                    print(result)\n",
        "            except SyntaxError:\n",
        "                exec(code, self.globals_dict, self.locals_dict)\n",
        "\n",
        "            output = captured_output.getvalue()\n",
        "            error_output = captured_error.getvalue()\n",
        "\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "            self.execution_history.append({\n",
        "                'code': code,\n",
        "                'output': output,\n",
        "                'result': execution_result,\n",
        "                'error': error_output\n",
        "            })\n",
        "\n",
        "            response = f\"**Code Executed:**\\n```python\\n{code}\\n```\\n\\n\"\n",
        "            if error_output:\n",
        "                response += f\"**Errors/Warnings:**\\n{error_output}\\n\\n\"\n",
        "            response += f\"**Output:**\\n{output if output.strip() else 'No console output'}\"\n",
        "\n",
        "            if execution_result is not None and not output.strip():\n",
        "                response += f\"\\n**Return Value:** {execution_result}\"\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            sys.stdout = old_stdout\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "            error_info = f\"**Code Executed:**\\n```python\\n{code}\\n```\\n\\n**Runtime Error:**\\n{str(e)}\\n**Error Type:** {type(e).__name__}\"\n",
        "\n",
        "            self.execution_history.append({\n",
        "                'code': code,\n",
        "                'output': '',\n",
        "                'result': None,\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "            return error_info\n",
        "\n",
        "    def get_execution_history(self) -> List[Dict[str, Any]]:\n",
        "        return self.execution_history\n",
        "\n",
        "    def clear_history(self):\n",
        "        self.execution_history = []"
      ],
      "metadata": {
        "id": "Q9gFnQkQXsd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResultValidator:\n",
        "    def __init__(self, python_repl: PythonREPLTool):\n",
        "        self.python_repl = python_repl\n",
        "\n",
        "    def validate_mathematical_result(self, description: str, expected_properties: Dict[str, Any]) -> str:\n",
        "        \"\"\"Validate mathematical computations\"\"\"\n",
        "        validation_code = f\"\"\"\n",
        "# Validation for: {description}\n",
        "validation_results = {{}}\n",
        "\n",
        "# Get the last execution results\n",
        "history = {self.python_repl.execution_history}\n",
        "if history:\n",
        "    last_execution = history[-1]\n",
        "    print(f\"Last execution output: {{last_execution['output']}}\")\n",
        "\n",
        "    # Extract numbers from the output\n",
        "    import re\n",
        "    numbers = re.findall(r'\\\\d+(?:\\\\.\\\\d+)?', last_execution['output'])\n",
        "    if numbers:\n",
        "        numbers = [float(n) for n in numbers]\n",
        "        validation_results['extracted_numbers'] = numbers\n",
        "\n",
        "        # Validate expected properties\n",
        "        for prop, expected_value in {expected_properties}.items():\n",
        "            if prop == 'count':\n",
        "                actual_count = len(numbers)\n",
        "                validation_results[f'count_check'] = actual_count == expected_value\n",
        "                print(f\"Count validation: Expected {{expected_value}}, Got {{actual_count}}\")\n",
        "            elif prop == 'max_value':\n",
        "                if numbers:\n",
        "                    max_val = max(numbers)\n",
        "                    validation_results[f'max_check'] = max_val <= expected_value\n",
        "                    print(f\"Max value validation: {{max_val}} <= {{expected_value}} = {{max_val <= expected_value}}\")\n",
        "            elif prop == 'min_value':\n",
        "                if numbers:\n",
        "                    min_val = min(numbers)\n",
        "                    validation_results[f'min_check'] = min_val >= expected_value\n",
        "                    print(f\"Min value validation: {{min_val}} >= {{expected_value}} = {{min_val >= expected_value}}\")\n",
        "            elif prop == 'sum_range':\n",
        "                if numbers:\n",
        "                    total = sum(numbers)\n",
        "                    min_sum, max_sum = expected_value\n",
        "                    validation_results[f'sum_check'] = min_sum <= total <= max_sum\n",
        "                    print(f\"Sum validation: {{min_sum}} <= {{total}} <= {{max_sum}} = {{min_sum <= total <= max_sum}}\")\n",
        "\n",
        "print(\"\\\\nValidation Summary:\")\n",
        "for key, value in validation_results.items():\n",
        "    print(f\"{{key}}: {{value}}\")\n",
        "\n",
        "validation_results\n",
        "\"\"\"\n",
        "        return self.python_repl.run(validation_code)\n",
        "\n",
        "    def validate_data_analysis(self, description: str, expected_structure: Dict[str, Any]) -> str:\n",
        "        \"\"\"Validate data analysis results\"\"\"\n",
        "        validation_code = f\"\"\"\n",
        "# Data Analysis Validation for: {description}\n",
        "validation_results = {{}}\n",
        "\n",
        "# Check if required variables exist in global scope\n",
        "required_vars = {list(expected_structure.keys())}\n",
        "existing_vars = []\n",
        "\n",
        "for var_name in required_vars:\n",
        "    if var_name in globals():\n",
        "        existing_vars.append(var_name)\n",
        "        var_value = globals()[var_name]\n",
        "        validation_results[f'{{var_name}}_exists'] = True\n",
        "        validation_results[f'{{var_name}}_type'] = type(var_value).__name__\n",
        "\n",
        "        # Type-specific validations\n",
        "        if isinstance(var_value, (list, tuple)):\n",
        "            validation_results[f'{{var_name}}_length'] = len(var_value)\n",
        "        elif isinstance(var_value, dict):\n",
        "            validation_results[f'{{var_name}}_keys'] = list(var_value.keys())\n",
        "        elif isinstance(var_value, (int, float)):\n",
        "            validation_results[f'{{var_name}}_value'] = var_value\n",
        "\n",
        "        print(f\"âœ“ Variable '{{var_name}}' found: {{type(var_value).__name__}} = {{var_value}}\")\n",
        "    else:\n",
        "        validation_results[f'{{var_name}}_exists'] = False\n",
        "        print(f\"âœ— Variable '{{var_name}}' not found\")\n",
        "\n",
        "print(f\"\\\\nFound {{len(existing_vars)}}/{{len(required_vars)}} required variables\")\n",
        "\n",
        "# Additional structure validation\n",
        "for var_name, expected_type in {expected_structure}.items():\n",
        "    if var_name in globals():\n",
        "        actual_type = type(globals()[var_name]).__name__\n",
        "        validation_results[f'{{var_name}}_type_match'] = actual_type == expected_type\n",
        "        print(f\"Type check '{{var_name}}': Expected {{expected_type}}, Got {{actual_type}}\")\n",
        "\n",
        "validation_results\n",
        "\"\"\"\n",
        "        return self.python_repl.run(validation_code)\n",
        "\n",
        "    def validate_algorithm_correctness(self, description: str, test_cases: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Validate algorithm implementations with test cases\"\"\"\n",
        "        validation_code = f\"\"\"\n",
        "# Algorithm Validation for: {description}\n",
        "validation_results = {{}}\n",
        "test_results = []\n",
        "\n",
        "test_cases = {test_cases}\n",
        "\n",
        "for i, test_case in enumerate(test_cases):\n",
        "    test_name = test_case.get('name', f'Test {{i+1}}')\n",
        "    input_val = test_case.get('input')\n",
        "    expected = test_case.get('expected')\n",
        "    function_name = test_case.get('function')\n",
        "\n",
        "    print(f\"\\\\nRunning {{test_name}}:\")\n",
        "    print(f\"Input: {{input_val}}\")\n",
        "    print(f\"Expected: {{expected}}\")\n",
        "\n",
        "    try:\n",
        "        if function_name and function_name in globals():\n",
        "            func = globals()[function_name]\n",
        "            if callable(func):\n",
        "                if isinstance(input_val, (list, tuple)):\n",
        "                    result = func(*input_val)\n",
        "                else:\n",
        "                    result = func(input_val)\n",
        "\n",
        "                passed = result == expected\n",
        "                test_results.append({{\n",
        "                    'test_name': test_name,\n",
        "                    'input': input_val,\n",
        "                    'expected': expected,\n",
        "                    'actual': result,\n",
        "                    'passed': passed\n",
        "                }})\n",
        "\n",
        "                status = \"âœ“ PASS\" if passed else \"âœ— FAIL\"\n",
        "                print(f\"Actual: {{result}}\")\n",
        "                print(f\"Status: {{status}}\")\n",
        "            else:\n",
        "                print(f\"âœ— ERROR: '{{function_name}}' is not callable\")\n",
        "        else:\n",
        "            print(f\"âœ— ERROR: Function '{{function_name}}' not found\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— ERROR: {{str(e)}}\")\n",
        "        test_results.append({{\n",
        "            'test_name': test_name,\n",
        "            'error': str(e),\n",
        "            'passed': False\n",
        "        }})\n",
        "\n",
        "# Summary\n",
        "passed_tests = sum(1 for test in test_results if test.get('passed', False))\n",
        "total_tests = len(test_results)\n",
        "validation_results['tests_passed'] = passed_tests\n",
        "validation_results['total_tests'] = total_tests\n",
        "validation_results['success_rate'] = passed_tests / total_tests if total_tests > 0 else 0\n",
        "\n",
        "print(f\"\\\\n=== VALIDATION SUMMARY ===\")\n",
        "print(f\"Tests passed: {{passed_tests}}/{{total_tests}}\")\n",
        "print(f\"Success rate: {{validation_results['success_rate']:.1%}}\")\n",
        "\n",
        "test_results\n",
        "\"\"\"\n",
        "        return self.python_repl.run(validation_code)"
      ],
      "metadata": {
        "id": "iv4mzqRCXorI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python_repl = PythonREPLTool()\n",
        "validator = ResultValidator(python_repl)"
      ],
      "metadata": {
        "id": "RBv-hN26Xx5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python_tool = Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"Execute Python code and return both the code and its output. Maintains state between executions.\",\n",
        "    func=python_repl.run\n",
        ")\n",
        "\n",
        "validation_tool = Tool(\n",
        "    name=\"result_validator\",\n",
        "    description=\"Validate the results of previous computations with specific test cases and expected properties.\",\n",
        "    func=lambda query: validator.validate_mathematical_result(query, {})\n",
        ")"
      ],
      "metadata": {
        "id": "WO_CjgiXXzJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"You are Claude, an advanced AI assistant with Python execution and result validation capabilities.\n",
        "\n",
        "You can execute Python code to solve complex problems and then validate your results to ensure accuracy.\n",
        "\n",
        "Available tools:\n",
        "{tools}\n",
        "\n",
        "Use this format:\n",
        "Question: the input question you must answer\n",
        "Thought: analyze what needs to be done\n",
        "Action: {tool_names}\n",
        "Action Input: [your input]\n",
        "Observation: [result]\n",
        "... (repeat Thought/Action/Action Input/Observation as needed)\n",
        "Thought: I should validate my results\n",
        "Action: [validation if needed]\n",
        "Action Input: [validation parameters]\n",
        "Observation: [validation results]\n",
        "Thought: I now have the complete answer\n",
        "Final Answer: [comprehensive answer with validation confirmation]\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"input\", \"agent_scratchpad\"],\n",
        "    partial_variables={\n",
        "        \"tools\": \"python_repl - Execute Python code\\nresult_validator - Validate computation results\",\n",
        "        \"tool_names\": \"python_repl, result_validator\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "7yZdUe9mX1PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedClaudeCodeAgent:\n",
        "    def __init__(self, anthropic_api_key=None):\n",
        "        if anthropic_api_key:\n",
        "            os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_api_key\n",
        "\n",
        "        self.llm = ChatAnthropic(\n",
        "            model=\"claude-3-opus-20240229\",\n",
        "            temperature=0,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "\n",
        "        self.agent = create_react_agent(\n",
        "            llm=self.llm,\n",
        "            tools=[python_tool, validation_tool],\n",
        "            prompt=prompt\n",
        "        )\n",
        "\n",
        "        self.agent_executor = AgentExecutor(\n",
        "            agent=self.agent,\n",
        "            tools=[python_tool, validation_tool],\n",
        "            verbose=True,\n",
        "            handle_parsing_errors=True,\n",
        "            max_iterations=8,\n",
        "            return_intermediate_steps=True\n",
        "        )\n",
        "\n",
        "        self.python_repl = python_repl\n",
        "        self.validator = validator\n",
        "\n",
        "    def run(self, query: str) -> str:\n",
        "        try:\n",
        "            result = self.agent_executor.invoke({\"input\": query})\n",
        "            return result[\"output\"]\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    def validate_last_result(self, description: str, validation_params: Dict[str, Any]) -> str:\n",
        "        \"\"\"Manually validate the last computation result\"\"\"\n",
        "        if 'test_cases' in validation_params:\n",
        "            return self.validator.validate_algorithm_correctness(description, validation_params['test_cases'])\n",
        "        elif 'expected_structure' in validation_params:\n",
        "            return self.validator.validate_data_analysis(description, validation_params['expected_structure'])\n",
        "        else:\n",
        "            return self.validator.validate_mathematical_result(description, validation_params)\n",
        "\n",
        "    def get_execution_summary(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get summary of all executions\"\"\"\n",
        "        history = self.python_repl.get_execution_history()\n",
        "        return {\n",
        "            'total_executions': len(history),\n",
        "            'successful_executions': len([h for h in history if not h['error']]),\n",
        "            'failed_executions': len([h for h in history if h['error']]),\n",
        "            'execution_details': history\n",
        "        }"
      ],
      "metadata": {
        "id": "0i3CZ0giX4AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06JqEiPoT-r1",
        "outputId": "f4949df0-3b65-41da-d3dc-8cf3b7bfa93a"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    API_KEY = \"Use Your Own Key Here\"\n",
        "\n",
        "    agent = AdvancedClaudeCodeAgent(anthropic_api_key=API_KEY)\n",
        "\n",
        "    print(\"ðŸš€ Advanced Claude Code Agent with Validation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"\\nðŸ”¢ Example 1: Prime Number Analysis with Twin Prime Detection\")\n",
        "    print(\"-\" * 60)\n",
        "    query1 = \"\"\"\n",
        "    Find all prime numbers between 1 and 200, then:\n",
        "    1. Calculate their sum\n",
        "    2. Find all twin prime pairs (primes that differ by 2)\n",
        "    3. Calculate the average gap between consecutive primes\n",
        "    4. Identify the largest prime gap in this range\n",
        "    After computation, validate that we found the correct number of primes and that all identified numbers are actually prime.\n",
        "    \"\"\"\n",
        "    result1 = agent.run(query1)\n",
        "    print(result1)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "    print(\"ðŸ“Š Example 2: Advanced Sales Data Analysis with Statistical Validation\")\n",
        "    print(\"-\" * 60)\n",
        "    query2 = \"\"\"\n",
        "    Create a comprehensive sales analysis:\n",
        "    1. Generate sales data for 12 products across 24 months with realistic seasonal patterns\n",
        "    2. Calculate monthly growth rates, yearly totals, and trend analysis\n",
        "    3. Identify top 3 performing products and worst 3 performing products\n",
        "    4. Perform correlation analysis between different products\n",
        "    5. Create summary statistics (mean, median, standard deviation, percentiles)\n",
        "    After analysis, validate the data structure, ensure all calculations are mathematically correct, and verify the statistical measures.\n",
        "    \"\"\"\n",
        "    result2 = agent.run(query2)\n",
        "    print(result2)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "    print(\"âš™ï¸ Example 3: Advanced Algorithm Implementation with Test Suite\")\n",
        "    print(\"-\" * 60)\n",
        "    query3 = \"\"\"\n",
        "    Implement and validate a comprehensive sorting and searching system:\n",
        "    1. Implement quicksort, mergesort, and binary search algorithms\n",
        "    2. Create test data with various edge cases (empty lists, single elements, duplicates, sorted/reverse sorted)\n",
        "    3. Benchmark the performance of different sorting algorithms\n",
        "    4. Implement a function to find the kth largest element using different approaches\n",
        "    5. Test all implementations with comprehensive test cases including edge cases\n",
        "    After implementation, validate each algorithm with multiple test cases to ensure correctness.\n",
        "    \"\"\"\n",
        "    result3 = agent.run(query3)\n",
        "    print(result3)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "    print(\"ðŸ¤– Example 4: Machine Learning Model with Cross-Validation\")\n",
        "    print(\"-\" * 60)\n",
        "    query4 = \"\"\"\n",
        "    Build a complete machine learning pipeline:\n",
        "    1. Generate a synthetic dataset with features and target variable (classification problem)\n",
        "    2. Implement data preprocessing (normalization, feature scaling)\n",
        "    3. Implement a simple linear classifier from scratch (gradient descent)\n",
        "    4. Split data into train/validation/test sets\n",
        "    5. Train the model and evaluate performance (accuracy, precision, recall)\n",
        "    6. Implement k-fold cross-validation\n",
        "    7. Compare results with different hyperparameters\n",
        "    Validate the entire pipeline by ensuring mathematical correctness of gradient descent, proper data splitting, and realistic performance metrics.\n",
        "    \"\"\"\n",
        "    result4 = agent.run(query4)\n",
        "    print(result4)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "    print(\"ðŸ“‹ Execution Summary\")\n",
        "    print(\"-\" * 60)\n",
        "    summary = agent.get_execution_summary()\n",
        "    print(f\"Total code executions: {summary['total_executions']}\")\n",
        "    print(f\"Successful executions: {summary['successful_executions']}\")\n",
        "    print(f\"Failed executions: {summary['failed_executions']}\")\n",
        "\n",
        "    if summary['failed_executions'] > 0:\n",
        "        print(\"\\nFailed executions details:\")\n",
        "        for i, execution in enumerate(summary['execution_details']):\n",
        "            if execution['error']:\n",
        "                print(f\"  {i+1}. Error: {execution['error']}\")\n",
        "\n",
        "    print(f\"\\nSuccess rate: {(summary['successful_executions']/summary['total_executions']*100):.1f}%\")"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfaBXN0iFbGR",
        "outputId": "78054492-6938-4b5c-8856-4f8740bfaefd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import json\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any\n",
        "from enum import Enum\n",
        "import random\n",
        "import re\n",
        "\n",
        "API_KEY = \"Use Your Own API Key\"\n",
        "genai.configure(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "SCDlCBgPFcqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MessageType(Enum):\n",
        "    HANDSHAKE = \"handshake\"\n",
        "    TASK_PROPOSAL = \"task_proposal\"\n",
        "    ANALYSIS = \"analysis\"\n",
        "    CRITIQUE = \"critique\"\n",
        "    SYNTHESIS = \"synthesis\"\n",
        "    VOTE = \"vote\"\n",
        "    CONSENSUS = \"consensus\""
      ],
      "metadata": {
        "id": "uS90QtB3FhLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class A2AMessage:\n",
        "    sender_id: str\n",
        "    receiver_id: str\n",
        "    message_type: MessageType\n",
        "    payload: Dict[str, Any]\n",
        "    timestamp: float\n",
        "    priority: int = 1"
      ],
      "metadata": {
        "id": "rEYN3TuAFnD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiAgent:\n",
        "    def __init__(self, agent_id: str, role: str, personality: str, temperature: float = 0.7):\n",
        "        self.agent_id = agent_id\n",
        "        self.role = role\n",
        "        self.personality = personality\n",
        "        self.temperature = temperature\n",
        "        self.conversation_memory = []\n",
        "        self.current_position = None\n",
        "        self.confidence = 0.5\n",
        "\n",
        "        self.model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "    def get_system_context(self, task_context: str = \"\") -> str:\n",
        "        return f\"\"\"You are {self.agent_id}, an AI agent in a multi-agent collaborative system.\n",
        "\n",
        "ROLE: {self.role}\n",
        "PERSONALITY: {self.personality}\n",
        "\n",
        "CONTEXT: {task_context}\n",
        "\n",
        "You are participating in Agent2Agent protocol communication. Your responsibilities:\n",
        "1. Analyze problems from your specialized perspective\n",
        "2. Provide constructive feedback to other agents\n",
        "3. Synthesize information from multiple sources\n",
        "4. Make data-driven decisions\n",
        "5. Collaborate effectively while maintaining your expertise\n",
        "\n",
        "IMPORTANT: Always structure your response as JSON with these fields:\n",
        "{{\n",
        "    \"agent_id\": \"{self.agent_id}\",\n",
        "    \"main_response\": \"your primary response content\",\n",
        "    \"confidence_level\": 0.8,\n",
        "    \"key_insights\": [\"insight1\", \"insight2\"],\n",
        "    \"questions_for_others\": [\"question1\", \"question2\"],\n",
        "    \"next_action\": \"suggested next step\"\n",
        "}}\n",
        "\n",
        "Stay true to your role and personality while being collaborative.\"\"\"\n",
        "\n",
        "    def generate_response(self, prompt: str, context: str = \"\") -> Dict[str, Any]:\n",
        "        \"\"\"Generate response using Gemini API\"\"\"\n",
        "        try:\n",
        "            full_prompt = f\"{self.get_system_context(context)}\\n\\nPROMPT: {prompt}\"\n",
        "\n",
        "            response = self.model.generate_content(\n",
        "                full_prompt,\n",
        "                generation_config=genai.types.GenerationConfig(\n",
        "                    temperature=self.temperature,\n",
        "                    max_output_tokens=600,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            response_text = response.text\n",
        "\n",
        "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    return json.loads(json_match.group())\n",
        "                except json.JSONDecodeError:\n",
        "                    pass\n",
        "\n",
        "            return {\n",
        "                \"agent_id\": self.agent_id,\n",
        "                \"main_response\": response_text[:200] + \"...\" if len(response_text) > 200 else response_text,\n",
        "                \"confidence_level\": random.uniform(0.6, 0.9),\n",
        "                \"key_insights\": [f\"Insight from {self.role}\"],\n",
        "                \"questions_for_others\": [\"What do you think about this approach?\"],\n",
        "                \"next_action\": \"Continue analysis\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Gemini API Error for {self.agent_id}: {e}\")\n",
        "            return {\n",
        "                \"agent_id\": self.agent_id,\n",
        "                \"main_response\": f\"Error occurred in {self.agent_id}: {str(e)}\",\n",
        "                \"confidence_level\": 0.1,\n",
        "                \"key_insights\": [\"API error encountered\"],\n",
        "                \"questions_for_others\": [],\n",
        "                \"next_action\": \"Retry connection\"\n",
        "            }\n",
        "\n",
        "    def analyze_task(self, task: str) -> Dict[str, Any]:\n",
        "        prompt = f\"Analyze this task from your {self.role} perspective: {task}\"\n",
        "        return self.generate_response(prompt, f\"Task Analysis: {task}\")\n",
        "\n",
        "    def critique_analysis(self, other_analysis: Dict[str, Any], original_task: str) -> Dict[str, Any]:\n",
        "        analysis_summary = other_analysis.get('main_response', 'No analysis provided')\n",
        "        prompt = f\"\"\"\n",
        "        ORIGINAL TASK: {original_task}\n",
        "\n",
        "        ANOTHER AGENT'S ANALYSIS: {analysis_summary}\n",
        "        THEIR CONFIDENCE: {other_analysis.get('confidence_level', 0.5)}\n",
        "        THEIR INSIGHTS: {other_analysis.get('key_insights', [])}\n",
        "\n",
        "        Provide constructive critique and alternative perspectives from your {self.role} expertise.\n",
        "        \"\"\"\n",
        "        return self.generate_response(prompt, f\"Critique Session: {original_task}\")\n",
        "\n",
        "    def synthesize_solutions(self, all_analyses: List[Dict[str, Any]], task: str) -> Dict[str, Any]:\n",
        "        analyses_summary = \"\\n\".join([\n",
        "            f\"Agent {i+1}: {analysis.get('main_response', 'No response')[:100]}...\"\n",
        "            for i, analysis in enumerate(all_analyses)\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        TASK: {task}\n",
        "\n",
        "        ALL AGENT ANALYSES:\n",
        "        {analyses_summary}\n",
        "\n",
        "        As the {self.role}, synthesize these perspectives into a comprehensive solution.\n",
        "        Identify common themes, resolve conflicts, and propose the best path forward.\n",
        "        \"\"\"\n",
        "        return self.generate_response(prompt, f\"Synthesis Phase: {task}\")"
      ],
      "metadata": {
        "id": "pq9SV7sBFyUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent2AgentCollaborativeSystem:\n",
        "    def __init__(self):\n",
        "        self.agents: Dict[str, GeminiAgent] = {}\n",
        "        self.collaboration_history: List[Dict[str, Any]] = []\n",
        "\n",
        "    def add_agent(self, agent: GeminiAgent):\n",
        "        self.agents[agent.agent_id] = agent\n",
        "        print(f\"ü§ñ Registered Gemini Agent: {agent.agent_id} ({agent.role})\")\n",
        "\n",
        "    def run_collaborative_problem_solving(self, problem: str):\n",
        "        print(f\"\\nüéØ Multi-Gemini Collaborative Problem Solving\")\n",
        "        print(f\"üîç Problem: {problem}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        print(\"\\nüìä PHASE 1: Individual Agent Analysis\")\n",
        "        initial_analyses = {}\n",
        "\n",
        "        for agent_id, agent in self.agents.items():\n",
        "            print(f\"\\nüß† {agent_id} analyzing...\")\n",
        "            analysis = agent.analyze_task(problem)\n",
        "            initial_analyses[agent_id] = analysis\n",
        "\n",
        "            print(f\"‚úÖ {agent_id} ({agent.role}):\")\n",
        "            print(f\"   Response: {analysis.get('main_response', 'No response')[:150]}...\")\n",
        "            print(f\"   Confidence: {analysis.get('confidence_level', 0.5):.2f}\")\n",
        "            print(f\"   Key Insights: {analysis.get('key_insights', [])}\")\n",
        "\n",
        "        print(f\"\\nüîÑ PHASE 2: Cross-Agent Critique & Feedback\")\n",
        "        critiques = {}\n",
        "\n",
        "        agent_list = list(self.agents.items())\n",
        "        for i, (agent_id, agent) in enumerate(agent_list):\n",
        "            target_agent_id = agent_list[(i + 1) % len(agent_list)][0]\n",
        "            target_analysis = initial_analyses[target_agent_id]\n",
        "\n",
        "            print(f\"\\nüîç {agent_id} critiquing {target_agent_id}'s analysis...\")\n",
        "            critique = agent.critique_analysis(target_analysis, problem)\n",
        "            critiques[f\"{agent_id}_critiques_{target_agent_id}\"] = critique\n",
        "\n",
        "            print(f\"üí¨ {agent_id} ‚Üí {target_agent_id}:\")\n",
        "            print(f\"   Critique: {critique.get('main_response', 'No critique')[:120]}...\")\n",
        "            print(f\"   Questions: {critique.get('questions_for_others', [])}\")\n",
        "\n",
        "        print(f\"\\nüî¨ PHASE 3: Solution Synthesis\")\n",
        "        final_solutions = {}\n",
        "        all_analyses = list(initial_analyses.values())\n",
        "\n",
        "        for agent_id, agent in self.agents.items():\n",
        "            print(f\"\\nüéØ {agent_id} synthesizing final solution...\")\n",
        "            synthesis = agent.synthesize_solutions(all_analyses, problem)\n",
        "            final_solutions[agent_id] = synthesis\n",
        "\n",
        "            print(f\"üèÜ {agent_id} Final Solution:\")\n",
        "            print(f\"   {synthesis.get('main_response', 'No synthesis')[:200]}...\")\n",
        "            print(f\"   Confidence: {synthesis.get('confidence_level', 0.5):.2f}\")\n",
        "            print(f\"   Next Action: {synthesis.get('next_action', 'No action specified')}\")\n",
        "\n",
        "        print(f\"\\nü§ù PHASE 4: Consensus & Recommendation\")\n",
        "\n",
        "        avg_confidence = sum(\n",
        "            sol.get('confidence_level', 0.5) for sol in final_solutions.values()\n",
        "        ) / len(final_solutions)\n",
        "\n",
        "        print(f\"üìä Average Solution Confidence: {avg_confidence:.2f}\")\n",
        "\n",
        "        most_confident_agent = max(\n",
        "            final_solutions.items(),\n",
        "            key=lambda x: x[1].get('confidence_level', 0)\n",
        "        )\n",
        "\n",
        "        print(f\"\\nüèÖ Most Confident Solution from: {most_confident_agent[0]}\")\n",
        "        print(f\"üìù Recommended Solution: {most_confident_agent[1].get('main_response', 'No solution')}\")\n",
        "\n",
        "        all_insights = []\n",
        "        for solution in final_solutions.values():\n",
        "            all_insights.extend(solution.get('key_insights', []))\n",
        "\n",
        "        print(f\"\\nüí° Collective Intelligence Insights:\")\n",
        "        for i, insight in enumerate(set(all_insights), 1):\n",
        "            print(f\"   {i}. {insight}\")\n",
        "\n",
        "        return final_solutions"
      ],
      "metadata": {
        "id": "rJfhS5REFpLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_specialized_gemini_agents():\n",
        "    \"\"\"Create diverse Gemini agents with different roles and personalities\"\"\"\n",
        "    agents = [\n",
        "        GeminiAgent(\n",
        "            \"DataScientist_Alpha\",\n",
        "            \"Data Scientist & Analytics Specialist\",\n",
        "            \"Methodical, evidence-based, loves patterns and statistical insights\",\n",
        "            temperature=0.3\n",
        "        ),\n",
        "        GeminiAgent(\n",
        "            \"ProductManager_Beta\",\n",
        "            \"Product Strategy & User Experience Expert\",\n",
        "            \"User-focused, strategic thinker, balances business needs with user value\",\n",
        "            temperature=0.5\n",
        "        ),\n",
        "        GeminiAgent(\n",
        "            \"TechArchitect_Gamma\",\n",
        "            \"Technical Architecture & Engineering Lead\",\n",
        "            \"System-oriented, focuses on scalability, performance, and technical feasibility\",\n",
        "            temperature=0.4\n",
        "        ),\n",
        "        GeminiAgent(\n",
        "            \"CreativeInnovator_Delta\",\n",
        "            \"Innovation & Creative Problem Solving Specialist\",\n",
        "            \"Bold, unconventional, pushes boundaries and suggests breakthrough approaches\",\n",
        "            temperature=0.8\n",
        "        ),\n",
        "        GeminiAgent(\n",
        "            \"RiskAnalyst_Epsilon\",\n",
        "            \"Risk Management & Compliance Expert\",\n",
        "            \"Cautious, thorough, identifies potential issues and mitigation strategies\",\n",
        "            temperature=0.2\n",
        "        )\n",
        "    ]\n",
        "    return agents"
      ],
      "metadata": {
        "id": "ikPNaJYEF21k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gemini_agent2agent_demo():\n",
        "    print(\"üöÄ Agent2Agent Protocol: Multi-Gemini Collaborative Intelligence\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if API_KEY == \"your-gemini-api-key-here\":\n",
        "        print(\"‚ö†Ô∏è  Please set your Gemini API key!\")\n",
        "        print(\"üí° Get your free API key from: https://makersuite.google.com/app/apikey\")\n",
        "        return\n",
        "\n",
        "    collaborative_system = Agent2AgentCollaborativeSystem()\n",
        "\n",
        "    for agent in create_specialized_gemini_agents():\n",
        "        collaborative_system.add_agent(agent)\n",
        "\n",
        "    problems = [\n",
        "        \"Design a sustainable urban transportation system for a city of 2 million people that reduces carbon emissions by 50% while maintaining economic viability.\",\n",
        "        \"Create a strategy for a tech startup to compete against established players in the AI-powered healthcare diagnostics market.\"\n",
        "    ]\n",
        "\n",
        "    for i, problem in enumerate(problems, 1):\n",
        "        print(f\"\\n{'üåü COLLABORATION SESSION ' + str(i):=^80}\")\n",
        "        collaborative_system.run_collaborative_problem_solving(problem)\n",
        "\n",
        "        if i < len(problems):\n",
        "            print(f\"\\n{'‚è∏Ô∏è  BREAK BETWEEN SESSIONS':=^80}\")\n",
        "            time.sleep(3)\n",
        "\n",
        "    print(f\"\\nüéâ Multi-Gemini Agent2Agent Collaboration Complete!\")\n",
        "    print(\"üí° This demonstrates true AI-to-AI collaboration using Google's Gemini models!\")\n",
        "    print(\"ü§ñ Each agent brought unique expertise to solve complex problems collectively!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_gemini_agent2agent_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-hLVdi-EB1oh",
        "outputId": "f35de16d-3d4e-43db-db2e-c0d48a6b03f8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}